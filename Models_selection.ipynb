{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARIES NEEDED\n",
    "# %pip install numpy\n",
    "# %pip install matplotlib\n",
    "# %pip install tensorflow\n",
    "# %pip install scikit-learn\n",
    "\n",
    "# Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.activations import relu, softmax\n",
    "\n",
    "# Used to prepare the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Loading and splitting the data* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (70000, 28, 28)\n",
      "y shape: (70000,)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Combining training and test data\n",
    "x_original = np.concatenate((x_train, x_test), axis=0)\n",
    "y_original = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "print(\"x shape:\", x_original.shape)\n",
    "print(\"y shape:\", y_original.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Devide the data into 3 sets (training, cross validation and test) to compare each model performance later*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the training set (input) is: (49000, 28, 28)\n",
      "the shape of the training set (target) is: (49000,)\n",
      "\n",
      "the shape of the cross validation set (input) is: (14070, 28, 28)\n",
      "the shape of the cross validation set (target) is: (14070,)\n",
      "\n",
      "the shape of the test set (input) is: (6930, 28, 28)\n",
      "the shape of the test set (target) is: (6930,)\n"
     ]
    }
   ],
   "source": [
    "# Get 60% of the dataset as the training set. \n",
    "# Put the remaining 40% in temporary variables: x_ and y_.\n",
    "\n",
    "x_train, x_, y_train, y_ = train_test_split(x_original, y_original, test_size = 0.3, random_state=1)\n",
    "\n",
    "# Split the 40% subset above into two: \n",
    "# one half for cross validation and the other for the test set\n",
    "\n",
    "x_dev, x_test, y_dev, y_test = train_test_split(x_, y_, test_size = 0.33, random_state=1)\n",
    "\n",
    "del x_, y_\n",
    "\n",
    "print(f\"the shape of the training set (input) is: {x_train.shape}\")\n",
    "print(f\"the shape of the training set (target) is: {y_train.shape}\\n\")\n",
    "print(f\"the shape of the cross validation set (input) is: {x_dev.shape}\")\n",
    "print(f\"the shape of the cross validation set (target) is: {y_dev.shape}\\n\")\n",
    "print(f\"the shape of the test set (input) is: {x_test.shape}\")\n",
    "print(f\"the shape of the test set (target) is: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Normalize the data to improve performance and avoid biased weights*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "x_train_normalized = tf.keras.utils.normalize(x_train, axis = 1)\n",
    "x_dev_normalized = tf.keras.utils.normalize(x_dev, axis = 1)\n",
    "x_test_normalized = tf.keras.utils.normalize(x_test, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Models implementation and training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# MODELS IMPLEMENTATION\n",
    "\n",
    "# for consistent results, \n",
    "# If you remove it, the results will not be reproducible \n",
    "# setting the random seed is crucial for consistent outcomes across different runs.\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "# Large model with 3 layers \n",
    "model1 = Sequential(\n",
    "    [\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(units = 128, activation = 'relu', name = 'L1'),\n",
    "        Dense(units = 128, activation = 'relu', name = 'L2'),\n",
    "        Dense(units = 10, activation = 'softmax', name = 'L3')\n",
    "    ], name = 'Model_1'\n",
    ")\n",
    "\n",
    "# Medium model with 3 layers\n",
    "model2 = Sequential(\n",
    "    [\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(units = 25, activation = 'relu', name = 'L1'),\n",
    "        Dense(units = 15, activation = 'relu', name = 'L2'),\n",
    "        Dense(units = 10, activation = 'softmax', name = 'L3'),\n",
    "    ], name = 'Model_2'\n",
    ")\n",
    "\n",
    "# Small model with 2 layers\n",
    "model3 = Sequential(\n",
    "    [\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(units = 128, activation = 'relu', name = 'L1'),\n",
    "        Dense(units = 10, activation = 'softmax', name = 'L3')\n",
    "    ], name = 'Model_3'\n",
    ")\n",
    "\n",
    "# List that store all the models to evaluate them later\n",
    "models = [model1, model2, model3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model_1...\n",
      "Done!\n",
      "\n",
      "Training Model_2...\n",
      "Done!\n",
      "\n",
      "Training Model_3...\n",
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compiling all models\n",
    "for model in models:\n",
    "\n",
    "    # Setup the loss and optimizer\n",
    "    model.compile(\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "    )\n",
    "\n",
    "    print(f\"Training {model.name}...\")\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        x_train_normalized, y_train,\n",
    "        epochs = 60,\n",
    "        verbose = 0   # Silent mode, no output during training.\n",
    "    )\n",
    "    \n",
    "    print(\"Done!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Evaluating the models* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*First compare the training error with the cross validation error and choose the one with the smallest cross validation error but also  avoiding overfitting in case the training error is to small comparing with the cross validation error*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1532/1532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m1532/1532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\u001b[1m1532/1532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m440/440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\n",
      "RESULTS:\n",
      "Model 1: Training Error: 0.06%, Cross validation Error: 2.51%\n",
      "Model 2: Training Error: 0.93%, Cross validation Error: 5.47%\n",
      "Model 3: Training Error: 0.00%, Cross validation Error: 2.74%\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists that will contain the errors for each model\n",
    "nn_train_errors = []\n",
    "nn_cv_errors = []\n",
    "\n",
    "# Calculate the errors\n",
    "for model in models:\n",
    "    # Calculate the training error and store it\n",
    "    train_predictions = model.predict(x_train_normalized)\n",
    "    train_accuracy = accuracy_score(y_train, train_predictions.argmax(axis=1))\n",
    "    nn_train_errors.append(1 - train_accuracy)\n",
    "\n",
    "    # Calculate the cross-validation error and store it\n",
    "    dev_predictions = model.predict(x_dev_normalized)\n",
    "    dev_accuracy = accuracy_score(y_dev, dev_predictions.argmax(axis=1))\n",
    "    nn_cv_errors.append(1 - dev_accuracy)\n",
    "\n",
    "# Comparing the errors for all models\n",
    "print(\"\\nRESULTS:\")\n",
    "\n",
    "for idx, (train_error, dev_error) in enumerate(zip(nn_train_errors, nn_cv_errors), 1):\n",
    "    print(f\"Model {idx}: Training Error: {train_error:.2%}, Cross validation Error: {dev_error:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Even when the cross-validation error of model 3 is the smallest, the perfect training error indicates an issue of overfitting. This is why model 1 emerges as the optimal choice.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Chosing a model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For illustrative purposes in this notebook we test all the models with the test set, but in practice this can be done only with the chossed model based on performance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "fraction of misclassified data for Model_1: 0.023520923520923522\n",
      "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "fraction of misclassified data for Model_2: 0.05800865800865801\n",
      "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "fraction of misclassified data for Model_3: 0.028715728715728715\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "\n",
    "    # Predict class probabilities for the training set using models\n",
    "    predictions = model.predict(x_test_normalized)\n",
    "\n",
    "    # Get the predicted class labels\n",
    "    yhat = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Initialize counter for misclassified data\n",
    "    misclassified = 0\n",
    "\n",
    "    for i in range (len(predictions)):\n",
    "        # Check if it matches the true labels\n",
    "        if yhat[i] != y_test[i]:\n",
    "            \n",
    "            # Add one to the counter if the prediction is wrong\n",
    "            misclassified += 1\n",
    "\n",
    "    # Compute the fraction of the data that the model misclassified\n",
    "    fraction_error = misclassified / len(predictions)\n",
    "    \n",
    "    print(f\"fraction of misclassified data for {model.name}: {fraction_error}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As expected model 1 show the best performance in the test set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the best model based on the performance over the test set\n",
    "# uncomment the line below to save the model\n",
    "\n",
    "#model1.save('M1_digit_rec.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
